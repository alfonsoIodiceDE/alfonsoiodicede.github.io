<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Biplots in dimension reduction and clustering</title>
    <meta charset="utf-8" />
    <meta name="author" content="Alfonso Iodice D’Enza, Michel van de Velden and Angelos Markos" />
    <meta name="date" content="2022-07-21" />
    <script src="biplots_in_dm_clust_IFCS22_files/header-attrs/header-attrs.js"></script>
    <link href="biplots_in_dm_clust_IFCS22_files/animate.css/animate.xaringan.css" rel="stylesheet" />
    <link href="biplots_in_dm_clust_IFCS22_files/tile-view/tile-view.css" rel="stylesheet" />
    <script src="biplots_in_dm_clust_IFCS22_files/tile-view/tile-view.js"></script>
    <link rel="stylesheet" href="xaringan-themer.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Biplots in dimension reduction and clustering
### Alfonso Iodice D’Enza, Michel van de Velden and Angelos Markos
### 21 July 2022

---







class: animated fadeIn 

### stacked data reduction (stacked DR)
- &lt;h4 style = "color:#37499c"&gt; variables and sample reduction &lt;/h4&gt;

- &lt;h4 style = "color:#37499c"&gt; the tandem approach and  cluster masking problem &lt;/h4&gt;

--

### joint DR
- &lt;h4 style = "color:#37499c"&gt; a unified framework for continuous, categorical and mixed data &lt;/h4&gt;

--

### biplots in joint DR for cluster characterization

- &lt;h4 style = "color:#37499c"&gt; discriminant analysis biplot, contribution biplot &lt;/h4&gt;


---
class: animated fadeIn inverse center middle

## stacked DR
### also known as 
###  the tandem approach

---
class: animated fadeIn middle

### data reduction paradigm

 **summarising** a two-way data matrix by **aggregating measurements** 
(Farcomeni and Greco, 2016)

- column-wise reduction: define a limited number of linear combinations (**dimension reduction**)

- row-wise reduction: define a limited number of observations, each representative of an homogeneous group (**partitioning clustering**)

---
class: animated fadeIn
### continuous data case: principal component analysis 

`\({\bf X}\)` is the `\(n \times p\)` standardized data matrix

The PCA loss function can be defined as:

**
`$$\min_{\mathbf{A,B}}\left\Vert \mathbf{Y}-n^{1/2}\mathbf{AB}^{\sf T}p^{1/2}\right\Vert ^{2} \ \ s.t. \ \
p\mathbf{B}^{\sf T}\mathbf{B}=\mathbf{I}_{d}$$`
**

where `\(\mathbf{Y}=n^{-1/2}\mathbf{X}p^{-1/2}\)`.

--

- the columns of `\(\mathbf{A}=n^{1/2}\mathbf{\tilde{U}{\bf \tilde{D}}_{\alpha}}\)` are the row (principal) coordinates, and they are such that
`\(\frac{1}{n}\mathbf{A}^{\sf T}\mathbf{A}={\bf \tilde{D}}_{\alpha}^{2}\)`

- the columns of `\(\mathbf{B}=p^{1/2}{\bf \tilde{V}}\)` are the column (standard) coordinates

--

Since `\({\bf \tilde{U}}{\bf \tilde{D}_{\alpha}}{\bf \tilde{V}^{{\sf T}}}\)` is the `\(d\)`-truncated SVD of `\(\bf{Y}\)`, then
`\(n^{1/2}\mathbf{AB}^{\sf T}p^{1/2}\)` is the best rank- `\(d\)` approximation of `\(\bf{Y}\)`, in the least square sense.


---
class: animated fadeIn
### categorical data case:  correspondence analysis

`\({\bf P}\)` is a two-way table with relative frequencies, crossing two categorical variables, with  `\(q_{r}\)` and `\(q_{c}\)` categories

The CA loss function is:

**
`$$\min_{\mathbf{A,B}}\left\Vert \mathbf{\tilde{P}-D}_{r}^{1/2}\mathbf{AB}^{\sf T}\mathbf{D}_{c}^{1/2}\right\Vert ^{2} \ \ s.t. \ \
\mathbf{B}^{\sf T}\mathbf{D}_{c}\mathbf{B}=\mathbf{I}_{d}$$`
**

where `\(\mathbf{\tilde{P}=D}_{r}^{-1/2}\left(\mathbf{P}-\mathbf{rc}^{\sf T}\right)\mathbf{D}_{c}^{-1/2}\)` , `\(\mathbf{r=P1}_{q_c}\)` , `\(\mathbf{c} =\mathbf{P}^{\sf T}\mathbf{1}_{q_r}\)`,
`\(\mathbf{D}_{r}=diag(\mathbf{r})\)`, `\(\mathbf{D}_{c} = diag(\mathbf{c})\)` 

--

- the columns of `\(\mathbf{A=D}_{r}^{-1/2}\mathbf{\tilde{U}{\bf \tilde{D}}_{\alpha}}\)` are the row (principal) coordinates, and they are such that
`\(\mathbf{A}^{\sf T}\mathbf{D}_{r}\mathbf{A={\bf D}_{\alpha}}^{2}\)`

- the columns of `\(\mathbf{B=D}_{c}^{-1/2}\mathbf{\tilde{V}}\)` are the column (standard) coordinates

--

Since `\({\bf \tilde{U}}{\bf \tilde{D}_{\alpha}}{\bf \tilde{V}^{{\sf T}}}\)` is the `\(d\)`-truncated SVD of `\(\bf{Y}\)`, then
`\(\mathbf{D}_{r}^{1/2}\mathbf{AB}^{\sf T}\mathbf{D}_{c}^{1/2}\)` is the best rank- `\(d\)` approximation of `\(\bf{Y}\)`, in the least square sense.



---
class: animated fadeIn
### categorical data case: (multiple) correspondence analysis

MCA generalizes the application of CA to `\(p\)` categorical variables, each with `\(q_{j}\)` categories, `\(j=1,\ldots,p\)`.

- `\({\bf Z}^{\star}_{j}\)` is the one-hot-encoding of the `\(j^{th}\)` categorical variable.

- `\({\bf Z}^{\star}=[{\bf Z}^{\star}_{1},\ldots,{\bf Z}^{\star}_{p}]\)` and `\({\bf Z}=\frac{{\bf Z}^{\star}}{n\times Q}\)`, with `\(Q=\sum_{j=1}^{p}{q_{j}}\)`; 

- the margins are `\({\bf r}=\frac{1}{n}{\bf 1}_{n}\)` and `\({\bf s}={\bf Z}^{{\sf T}}{\bf 1}_{n}\)`  

--

The (M)CA loss function is:

**
`$$\min_{\mathbf{A,B}}\left\Vert \mathbf{\tilde{Z}} - \frac{1}{\sqrt{n}}\mathbf{AB}^{\sf T}\mathbf{D}_{s}^{1/2}\right\Vert ^{2} \ \ s.t. \ \
\mathbf{B}^{\sf T}\mathbf{D}_{s}\mathbf{B}=\mathbf{I}_{d}$$`
**

where `\(\mathbf{\tilde{Z}}=\sqrt{n}\left(\mathbf{Z}-\frac{1}{n}{\bf 1}_{n}{\bf 1}_{n}^{{\sf T}}{\bf Z}\right)\mathbf{D}_{s}^{-1/2}\)`

---
class: animated fadeIn
### mixed data case: factor analysis of mixed data

Real datasets have often both continuous and categorical variables.

Upon an appropriate data pre-processing, dimension reduction is done via PCA.

--

Let `\(\bf X\)` contain the continuous variables (centered and standardised);

Let the `\(\bf Z\)` also be centered and standardized:

--

 - the centering operator is `\({\bf M} = {\bf I}_{n} - n^{-1}{\bf 1}_{n}{\bf 1}^{{\sf T}}_{n}\)` 
 - the scaling weights are in `\({\bf D}_{z}=diag({\bf Z}^{\sf T}{\bf Z})\)` 
 
The PCA of ** `\({\bf X^{\star}} = \left[{\bf X} \ {\bf D}_{z}^{-1/2}{\bf Z}{\bf M}\right]\)` ** is the FAMD

---
class: animated fadeIn
### partitioning cluster analysis

---
class: animated fadeIn
### curse of dimensionality?

&lt;img src="biplots_in_dm_clust_IFCS22_files/figure-html/unnamed-chunk-1-1.png" width="40%" style="display: block; margin: auto;" /&gt;

---
class: animated fadeIn
### curse of dimensionality?

&lt;img src="biplots_in_dm_clust_IFCS22_files/figure-html/unnamed-chunk-2-1.png" width="40%" style="display: block; margin: auto;" /&gt;



---
class: animated fadeIn
### curse of dimensionality?

&lt;img src="biplots_in_dm_clust_IFCS22_files/figure-html/unnamed-chunk-3-1.png" width="40%" style="display: block; margin: auto;" /&gt;

---
class: animated fadeIn
### curse of dimensionality?

&lt;img src="biplots_in_dm_clust_IFCS22_files/figure-html/unnamed-chunk-4-1.png" width="40%" style="display: block; margin: auto;" /&gt;


---
class: animated fadeIn
### curse of dimensionality?

&lt;img src="biplots_in_dm_clust_IFCS22_files/figure-html/unnamed-chunk-5-1.png" width="40%" style="display: block; margin: auto;" /&gt;

---
class: animated fadeIn
### stacking variables and samples reduction

Practitioners often apply dimension reduction and then cluster the low-dimensional scores: this approach is referred to as ** tandem analysis ** (Arabie and Hubert, 1996)

- mitigate the effects of the curse of dimensionality

- the graphical display of the dataset at hand may be of help for cluster characterization

- 


---
class: animated fadeIn
### cluster masking

Consider a toy example: three very well separated clusters in two dimensions





&lt;img src="./figures/pca_toy_anim.gif" width="45%" style="display: block; margin: auto;" /&gt;


---
class: animated fadeIn
### cluster masking

the PCA scores are easily clustered via, e.g., Kmeans

&lt;img src="biplots_in_dm_clust_IFCS22_files/figure-html/unnamed-chunk-8-1.png" width="45%" style="display: block; margin: auto;" /&gt;

---
class: animated fadeIn
### cluster masking

the toy example: adding 4 noise variables  `\(X_{j}\sim N(0,6), \ j=3,\ldots,6\)`. 


---
class: animated fadeIn
### cluster masking

the toy example: adding 4 noise variables  `\(X_{j}\sim N(0,6), \ j=3,\ldots,6\)`. 

&lt;img src="biplots_in_dm_clust_IFCS22_files/figure-html/unnamed-chunk-10-1.png" width="45%" style="display: block; margin: auto;" /&gt;

---
class: animated fadeIn
### cluster masking

The tandem approach: the 2-d PC map

&lt;img src="./figures/tandem_toy_anim.gif" width="45%" style="display: block; margin: auto;" /&gt;

---
class: animated fadeIn
### cluster masking

The tandem approach will fail: the 2-d PC map does not display clustered observations

&lt;img src="biplots_in_dm_clust_IFCS22_files/figure-html/unnamed-chunk-12-1.png" width="45%" style="display: block; margin: auto;" /&gt;
---
class: animated fadeIn
### cluster masking

The tandem approach will fail: K-means considering an increasing number of PCs

&lt;img src="biplots_in_dm_clust_IFCS22_files/figure-html/unnamed-chunk-13-1.png" width="45%" style="display: block; margin: auto;" /&gt;


---
class: animated fadeIn
### cluster masking

The tandem approach will fail: K-means considering an increasing number of PCs

&lt;img src="biplots_in_dm_clust_IFCS22_files/figure-html/unnamed-chunk-14-1.png" width="45%" style="display: block; margin: auto;" /&gt;

---
class: animated fadeIn
### cluster masking

The tandem approach will fail: K-means considering an increasing number of PCs

&lt;img src="biplots_in_dm_clust_IFCS22_files/figure-html/unnamed-chunk-15-1.png" width="45%" style="display: block; margin: auto;" /&gt;


---
class: animated fadeIn
### cluster masking

The tandem approach will fail: K-means considering an increasing number of PCs

&lt;img src="biplots_in_dm_clust_IFCS22_files/figure-html/unnamed-chunk-16-1.png" width="45%" style="display: block; margin: auto;" /&gt;


---
class: animated fadeIn inverse center middle

## joint data reduction (JDR)


---
class: animated fadeIn

### Beyond tandem analysis: joint methods

**Methods for continuous data**


- **Reduced K-means** (De Soete and Carroll, 1994): joint PCA + Kmeans
- **Factorial K-means** (Vichi and Kiers, 2001): joint PCA + Kmeans

**Methods for categorical data**


- **MCA-Kmeans** (Hwang, Dillon, and Takane, 2006): joint MCA + Kmeans
- **iFCB** (Iodice D’Enza and Palumbo, 2013): iterative sequential NSCA + Kmeans
- **Cluster Correspondence Analysis** (van de
Velden, Iodice D’Enza, and Palumbo, 2017): joint CA + Kmeans

**Methods for mixed data**

- cluster and dimension reduciton for mixed data (CDR, Vichi, Vicari, and Kiers, 2019)
- **Groupals** (CDR, Van Buuren and Heiser, 1989)


--
All methods are implemented in `\(\texttt{CRAN}\)` package `\(\texttt{clustrd}\)` (Markos, Iodice D'Enza, and van de Velden, 2019)

---
class: animated fadeIn
### JDR: a unified framework

The general objective can be formulated as follows (Yamamoto and Hwang, 2014; Vichi, Vicari, and Kiers, 2019):
 
 $$
 \min\phi\left(\mathbf{B},\mathbf{Z}_{K}\right)=
 \alpha\left\Vert\mathbf{X} - \mathbf{X}\mathbf{B}\mathbf{B}^{\prime}\right\Vert ^{2} +
 (1-\alpha)\left\Vert\mathbf{XB} - \mathbf{P}\mathbf{X}\mathbf{B}\right\Vert ^{2} 
 $$
 


`\(\mathbf{P} = \mathbf{Z}_{K}\left(\mathbf{Z}_{K}^\prime\mathbf{Z}_{K}\right)^{-1}\mathbf{Z}_{K}^\prime\)`: a projection matrix

`\({\bf G} = \mathbf{P}\mathbf{X}\mathbf{B}\)`: `\(K \times d\)` cluster centroid matrix

- `\(\alpha = 1/2\)`, **Reduced K-means**
- `\(\alpha = 0\)`, **Factorial K-means**

Note that `\(\alpha = 1\)` gives the PCA solution.

---
class: animated fadeIn
### JDR: a unified framework

$$
\min\phi\left(\mathbf{B},\mathbf{Z}_{K}\right)=
\alpha\left\Vert\mathbf{X} - \mathbf{X}\mathbf{B}\mathbf{B}^{\prime}\right\Vert ^{2} +
(1-\alpha)\left\Vert\mathbf{XB} - \mathbf{P}\mathbf{X}\mathbf{B}\right\Vert ^{2} \ \ \ (1)
$$

**Extensions to categorical data**

`$$\mathbf{X}^{cat} = \mathbf{D}_{z}^{-1/2}\mathbf{MZ}$$`

- `\(\alpha = 1/2\)`, Cluster Correspondence Analysis

**Extensions to mixed-type data**
`$$\mathbf{X}^{mix} = \left[\mathbf{X}^{cnt} \hspace{.35cm}  \mathbf{X}^{cat}\right]$$`

- `\(\alpha = 1/2\)`, Mixed Reduced K-means
- `\(\alpha = 0\)`, Mixed Factorial K-means

Note that `\(\alpha = 1\)` gives the PCAMIX/FAMD solution.

---
class: animated fadeIn
### JDR: a unified procedure


For given `\(\alpha\)`, the following ALS algorithm is used to minimize the loss function  (van de
Velden, Iodice D'Enza, and Markos, 2019):


- Generate an initial cluster allocation `\(\mathbf{Z}_{K}\)` (e.g., by randomly assigning subjects to clusters).
	
- Find loadings `\(\mathbf{B}\)` by taking the eigendecomposition of `\(\mathbf{X}^{\prime}\left((1-\alpha)\mathbf{P} - (1-2\alpha)\mathbf{I}\right)\mathbf{X}\)`
	
- Update the cluster allocation `\(\mathbf{Z}_{K}\)` by applying K-means to the reduced space subject coordinates `\(\mathbf{X}\mathbf{B}\)`
	
- Repeat the procedure (i.e. go back to step 2) using `\(\mathbf{Z}_{K}\)` for the cluster allocation matrix, until convergence.\ That is, until
	`\(\mathbf{Z}_{K}\)` remains constant.




---
class: animated fadeIn

### JDR on the toy data set: no more cluster masking

&lt;img src="biplots_in_dm_clust_IFCS22_files/figure-html/unnamed-chunk-17-1.png" width="45%" style="display: block; margin: auto;" /&gt;


---
class: animated fadeIn inverse center middle

## biplots in JDR
### for
### cluster characterization

---
class: animated fadeIn middle
### The Palmer Penguins data set (not iris)


---
class: animated fadeIn middle
### The Palmer Penguins: tandem 

&lt;img src="biplots_in_dm_clust_IFCS22_files/figure-html/unnamed-chunk-18-1.png" width="45%" style="display: block; margin: auto;" /&gt;

---
class: animated fadeIn 
### The Palmer Penguins: tandem to RKM


&lt;img src="./figures/tandem_to_rkm_anim.gif" width="45%" style="display: block; margin: auto;" /&gt;

---
class: animated fadeIn 
### The Palmer Penguins:  RKM


&lt;img src="biplots_in_dm_clust_IFCS22_files/figure-html/unnamed-chunk-20-1.png" width="45%" style="display: block; margin: auto;" /&gt;


---
class: animated fadeIn middle
### The zoo dataset
This is a dataset from the UCI repo (Newman, Hettich, Blake, and Merz, 1998; Leisch and Dimitriadou, 2021) 


- **82** animals of four types: **mammal**, **bird**, **fish** and **insect**

- **16** categorical variables describing the animals characteristics



---
class: animated fadeIn
### The zoo dataset





&lt;img src="biplots_in_dm_clust_IFCS22_files/figure-html/unnamed-chunk-23-1.png" width="45%" style="display: block; margin: auto;" /&gt;

---
class: animated fadeIn
### The zoo: asymmetric map to contribution biplot

&lt;img src="./figures/zoo_contr_anim.gif" width="45%" style="display: block; margin: auto;" /&gt;

---
class: animated fadeIn
### The zoo: asymmetric map to contribution biplot

&lt;img src="biplots_in_dm_clust_IFCS22_files/figure-html/unnamed-chunk-25-1.png" width="45%" style="display: block; margin: auto;" /&gt;



---
class: animated fadeIn

### References

Arabie, P. and L. Hubert (1996). "Advances in cluster analysis relevant
to marketing research". In: _From Data to Knowledge_. Springer, pp.
3-19.

De Soete, G. D. and J. D. Carroll (1994). "K-means clustering in a
low-dimensional Euclidean space". In: _New approaches in classification
and data analysis_. Springer, Berlin, Heidelberg, pp. 212-219.

Farcomeni, A. and L. Greco (2016). _Robust methods for data reduction_.
CRC press.

Hwang, H., W. R. Dillon, and Y. Takane (2006). "An extension of
multiple correspondence analysis for identifying heterogeneous
subgroups of respondents". In: _Psychometrika_ 71.1, pp. 161-171.

Iodice D’Enza, A. and F. Palumbo (2013). "Iterative factor clustering
of binary data". In: _Computational Statistics_ 28.2, pp. 789-807.

Leisch, F. and E. Dimitriadou (2021). _mlbench: Machine Learning
Benchmark Problems_. R package version 2.1-3.

Markos, A., A. Iodice D'Enza, and M. van de Velden (2019). "Beyond
Tandem Analysis: Joint Dimension Reduction and Clustering in R". In:
_Journal of Statistical Software_ 91.10, pp. 1-24. DOI:
[10.18637/jss.v091.i10](https://doi.org/10.18637%2Fjss.v091.i10).

---
class: animated fadeIn

### References (2)

Newman, D., S. Hettich, C. Blake, et al. (1998). _UCI Repository of
machine learning databases_. URL:
[http://www.ics.uci.edu/~mlearn/MLRepository.html](http://www.ics.uci.edu/~mlearn/MLRepository.html).

Van Buuren, S. and W. J. Heiser (1989). "Clusteringn objects intok
groups under optimal scaling of variables". In: _Psychometrika_ 54.4,
pp. 699-706.

Velden, M. van de, A. Iodice D'Enza, and A. Markos (2019).
"Distance-based clustering of mixed data". In: _Wiley Interdisciplinary
Reviews: Computational Statistics_ 11.3, p. e1456.

Velden, M. van de, A. Iodice D’Enza, and F. Palumbo (2017). "Cluster
correspondence analysis". In: _Psychometrika_ 82.1, pp. 158-185.

Vichi, M. and H. A. Kiers (2001). "Factorial k-means analysis for
two-way data". In: _Computational Statistics &amp; Data Analysis_ 37.1, pp.
49-64.

Vichi, M., D. Vicari, and H. A. Kiers (2019). "Clustering and dimension
reduction for mixed variables". In: _Behaviormetrika_ 46.2, pp.
243-269.

Yamamoto, M. and H. Hwang (2014). "A general formulation of cluster
analysis with dimension reduction and subspace separation". In:
_Behaviormetrika_ 41.1, pp. 115-129.

    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"navigation": {
"scroll": false
},
"slideNumberFormat": "%current%",
"highlightStyle": "github",
"highlightLines": true,
"ratio": "16:9",
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
